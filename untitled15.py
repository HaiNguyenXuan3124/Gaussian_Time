# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvwTXcWS3ugK2KE1qPy9jwRCgaOleDcr
"""

import pandas as pd
# Bước 1
df = pd.read_csv(
    '/content/GlobalLandTemperaturesByCountry.csv',
    parse_dates=['dt'],
    usecols=['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'Country']
)

# 2. Chuyển cột dt
df['Month'] = df['dt'].dt.to_period('M')
df = df.drop(columns='dt')

# 3. Lọc lấy Vietnam
country = 'Vietnam'
df_country = df[df['Country'] == country].copy()

# 4. Đặt index về Month và sắp xếp tăng dần
df_country = df_country.set_index('Month').sort_index()

# 5. Chọn chuỗi nhiệt độ kiểm tra dữ liệu thiếu
ts = df_country['AverageTemperature']
print(f"Missing before interpolation: {ts.isna().sum()}")

# 6. Nội suy theo thời gian
ts_interp = ts.interpolate(method='time')
print(f"Missing after interpolation:  {ts_interp.isna().sum()}")

# 7. Xem nhanh 5 giá trị đầu sau khi xử lý
print(ts_interp.head())

import pandas as pd
from sklearn.mixture import GaussianMixture

# Trích đặc trưng rolling mean và rolling std
df_features = pd.DataFrame({'temp': ts})
df_features['rolling_mean_12'] = df_features['temp'].rolling(window=12, min_periods=1).mean()
df_features['rolling_std_12'] = df_features['temp'].rolling(window=12, min_periods=1).std()

# Áp dụng GMM
K = 3
gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)
X = df_features[['rolling_mean_12', 'rolling_std_12']].dropna()
gmm.fit(X)

# Tạo embedding và nhãn cụm
probs = gmm.predict_proba(X)
labels = gmm.predict(X)
df_embedding = pd.DataFrame(probs, index=X.index, columns=[f'cluster_{i}' for i in range(K)])
df_embedding['label'] = labels
df_feat = pd.DataFrame({'temp': ts_interp})
df_feat['rolling_mean_12'] = df_feat['temp'].rolling(window=12, min_periods=1).mean()
df_feat['rolling_std_12'] = df_feat['temp'].rolling(window=12, min_periods=1).std()
K = 3
gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)
X_feats = df_feat[['rolling_mean_12', 'rolling_std_12']].dropna()
gmm.fit(X_feats)
probs = gmm.predict_proba(X_feats)
df_embedding = pd.DataFrame(probs, index=X_feats.index, columns=[f'cluster_{i}' for i in range(K)])

# Hiển thị 10 dòng đầu
print(df_embedding.head(10))

import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture

# Kết hợp chuỗi nhiệt độ và embedding
df_all = pd.concat([df_feat[['temp']], df_embedding], axis=1).dropna()

# Tạo windows cho Transformer
input_window = 36  # 36 tháng
forecast_horizon = 12  # dự báo 12 tháng

data = df_all.values  # shape: [T, 1 + K]
X_windows = []
Y_windows = []

for i in range(len(data) - input_window - forecast_horizon + 1):
    X_windows.append(data[i : i + input_window])
    Y_windows.append(data[i + input_window : i + input_window + forecast_horizon, 0])  # chỉ lấy temp

X = np.array(X_windows)  # shape: [num_samples, input_window, features]
Y = np.array(Y_windows)  # shape: [num_samples, forecast_horizon]

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import math

# Chuẩn bị dataset từ X và Y đã tạo
# Chuyển thành tensor
X_tensor = torch.tensor(X, dtype=torch.float32)
Y_tensor = torch.tensor(Y, dtype=torch.float32)

dataset = TensorDataset(X_tensor, Y_tensor)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

# Định nghĩa Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=500):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# Định nghĩa Transformer gốc cho forecasting
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_size, seq_len, label_len, out_len,
                 d_model=128, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=512, dropout=0.1):
        super().__init__()
        self.model_type = 'Transformer'
        self.d_model = d_model
        # input embedding
        self.input_proj = nn.Linear(input_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        # Transformer
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        # output projection
        self.output_proj = nn.Linear(d_model, 1)
        self.seq_len = seq_len
        self.label_len = label_len
        self.out_len = out_len

    def forward(self, src):
        # src: [batch, seq_len, features]
        batch_size = src.size(0)
        src_emb = self.input_proj(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoder(src_emb)

        # Prepare decoder input: use last label_len steps as start tokens
        # Here we use the last seq portion of temp only
        init_decoder_input = src_emb[:, -self.label_len:, :]
        tgt = init_decoder_input

        # Transformer forward
        output = self.transformer(src_emb, tgt)
        # output: [batch, label_len, d_model]
        # Project to single value and predict out_len values
        out = self.output_proj(output)  # [batch, label_len, 1]
        out = out[:, -self.out_len:, 0]  # [batch, out_len]
        return out

# Khởi tạo mô hình, loss, optimizer
seq_len = X.shape[1]
features = X.shape[2]
label_len = 12
out_len = Y.shape[1]

model = TimeSeriesTransformer(
    input_size=features,
    seq_len=seq_len,
    label_len=label_len,
    out_len=out_len
)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Vòng lặp huấn luyện đơn giản
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for xb, yb in dataloader:
        optimizer.zero_grad()
        pred = model(xb)
        loss = criterion(pred, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(dataloader)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}")

# Lưu mô hình
torch.save(model.state_dict(), 'transformer_ts_model.pth')
print("Training complete and model saved as 'transformer_ts_model.pth'")

import torch
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Load mô hình với đủ tham số
seq_len   = X.shape[1]
features  = X.shape[2]
label_len = 12
out_len   = 12

model = TimeSeriesTransformer(
    input_size=features,
    seq_len=seq_len,
    label_len=label_len,
    out_len=out_len
)
model.load_state_dict(torch.load('transformer_ts_model.pth', map_location='cpu'))
model.eval()

# Chia
n = X.shape[0]
test_size = int(0.2 * n)
X_test = X[-test_size:]
Y_test = Y[-test_size:]

# Dự báo
with torch.no_grad():
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    Y_pred = model(X_test_t).numpy()

# Tính metrics
mae  = mean_absolute_error(Y_test, Y_pred)
rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))
# Thêm epsilon để tránh chia cho 0
eps = 1e-8
mape = np.mean(np.abs((Y_test - Y_pred) / (Y_test + eps))) * 100

print(f"Test MAE : {mae:.4f}")
print(f"Test RMSE: {rmse:.4f}")
print(f"Test MAPE: {mape:.2f}%")

# Vẽ so sánh actual vs. predicted cho mẫu đầu tiên
plt.figure(figsize=(8,4))
plt.plot(range(1, out_len+1), Y_test[0],  marker='o', label='Actual')
plt.plot(range(1, out_len+1), Y_pred[0], marker='x', label='Predicted')
plt.xlabel('Tháng dự báo')
plt.ylabel('Temperature (°C)')
plt.title('Actual vs. Predicted (First Test Sample)')
plt.legend()
plt.grid(True)
plt.show()